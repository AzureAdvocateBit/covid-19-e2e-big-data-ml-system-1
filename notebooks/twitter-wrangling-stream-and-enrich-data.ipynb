{"cells":[{"cell_type":"markdown","source":["# Data Wrangling on Streaming data\nStream data from eventhubs into Azure Databricks\nLeverage Spark functionality to transform, clean and normalize the data\nto prepare it for Machine Learning modeling, tuning and classification"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Event Hubs Connection Configuration\n# for more deatils -> https://bit.ly/2Zw4qED\nehConf = {\n  'eventhubs.connectionString': dbutils.secrets.get(scope=\"mle2ebigdatakv\", key=\"twitterstreamingkey\") }"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["inputStream = spark.readStream.format(\"eventhubs\").options(**ehConf).load()\ninputStream = inputStream.withColumn(\"body\",inputStream[\"body\"].cast(\"string\"))\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Parse event body and set schema\nexpectedSchema = StructType([\n  StructField(\"user_name\", StringType(), True),\n  StructField(\"user_location\", StringType(), True),\n  StructField(\"user_description\", StringType(), True),\n  StructField(\"user_created\", StringType(), True),\n  StructField(\"user_followers\", FloatType(), True),\n  StructField(\"user_friends\", FloatType(), True),\n  StructField(\"user_favourites\", FloatType(), True),\n  StructField(\"user_verified\", BooleanType(), True),\n  StructField(\"date\", StringType(), True),\n  StructField(\"text\", StringType(), True),\n  StructField(\"hashtags\", StringType(), True),\n  StructField(\"source\", StringType(), True),\n  StructField(\"is_retweet\", BooleanType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# when using azure databricks, use this call to visualize the data\n#display(inputStream)\n\n# Split the body into an array\ninputStream = inputStream.select(\n  inputStream.enqueuedTime.alias('timestamp'),\n  split(inputStream.body.cast('string'), ',').alias('splitted_body')\n)\n\n# Map the body array to columns\nfor index, field in enumerate(expectedSchema):\n  inputStream = inputStream.withColumn(\n    field.name, inputStream.splitted_body.getItem(index)\n  )\n\n# Drop irrelevant columns\ninputStream = inputStream.drop('timestamp', 'splitted_body')\n\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["\n\n# Set data types - cast the data in columns to match schema\n\ninputStream = inputStream \\\n  .withColumn(\"user_name\", inputStream[\"user_name\"].cast(\"string\")) \\\n  .withColumn(\"user_location\", inputStream[\"user_location\"].cast(\"string\")) \\\n  .withColumn(\"user_description\", inputStream[\"user_description\"].cast(\"string\")) \\\n  .withColumn(\"user_created\", inputStream[\"user_created\"].cast(\"string\")) \\\n  .withColumn(\"user_followers\", inputStream[\"user_followers\"].cast(\"float\")) \\\n  .withColumn(\"user_friends\", inputStream[\"user_friends\"].cast(\"float\")) \\\n  .withColumn(\"user_favourites\", inputStream[\"user_favourites\"].cast(\"float\")) \\\n  .withColumn(\"user_verified\", inputStream[\"user_verified\"].cast(\"boolean\")) \\\n  .withColumn(\"date\", inputStream[\"date\"].cast(\"string\")) \\\n  .withColumn(\"text\", inputStream[\"text\"].cast(\"string\")) \\\n  .withColumn(\"hashtags\", inputStream[\"hashtags\"].cast(\"string\")) \\\n  .withColumn(\"source\", inputStream[\"source\"].cast(\"string\")) \\\n  .withColumn(\"is_retweet\", inputStream[\"is_retweet\"].cast(\"boolean\")) \\\n\n\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# provide endpoint and key \nsentimentEndpoint = dbutils.secrets.get(scope=\"mle2ebigdatakv\", key=\"sentimentEndpoint\")\nsentimentAccessKeys = dbutils.secrets.get(scope=\"mle2ebigdatakv\", key=\"sentimentAccessKeys\")\n\n# text analytics provides multiple tools for us to use, here we ask specificaly for sentiment\nlanguage_api_url = sentimentEndpoint + \"/text/analytics/v3.0/sentiment\" \nheaders = {\"Ocp-Apim-Subscription-Key\": sentimentAccessKeys}\n\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# drop null text values\ninputStream = inputStream.dropna(subset=('text'))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["import os\nfrom pyspark.sql.functions import udf\nimport pandas as pd\nimport requests\nfrom pprint import pprint\n\nlanguage_api_url = sentimentEndpoint + \"/text/analytics/v3.0/sentiment\"\nheaders = {\"Ocp-Apim-Subscription-Key\": sentimentAccessKeys}\n\nimport json\ndef constractDocRequest(text):\n  textJson = text\n  docRequest = {}\n  doc = {}\n  doc[\"id\"]= textJson\n  doc[\"text\"]= textJson\n  docRequest[\"documents\"] = [doc]\n  return docRequest\n\n\ndef extractSentiment(doc,sentimentType):\n  if doc == {} or not 'documents' in doc:\n    return 0.0\n  return float(doc['documents'][0]['confidenceScores'][sentimentType])\n\n\n\ndef getPositiveSentiment(text):\n  if bool(text.strip()) == False:\n    return 0.0\n  docRequest = constractDocRequest(text)\n  response = requests.post(language_api_url, headers=headers, json=docRequest)\n  sentiment = response.json()\n  positive = extractSentiment(sentiment,'positive')\n  return positive\n\nget_positive_sentiment = udf(getPositiveSentiment, StringType())\n\ninputStream = inputStream.withColumn('positive_sentiment', get_positive_sentiment(inputStream[\"text\"]))\n  \n\n\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#,Write processed streaming data to storage\n# Stream processed data to parquet for the Data Science to explore and build ML models\ncomments_stream.writeStream \\\n  .trigger(processingTime = \"30 seconds\") \\\n  .format(\"parquet\") \\\n  .outputMode(\"append\") \\\n  .partitionBy(\"user_location\") \\\n  .option(\"compression\", \"none\") \\\n  .option(\"checkpointLocation\", \"/mnt/stream/_checkpoints/covid19twitter\") \\\n  .start(\"/mnt/root/COVID19_TWEETS/REFINED/WITH_SENTIMENT/\")"],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"twitter-wrangling-stream","notebookId":1689419707369090},"nbformat":4,"nbformat_minor":0}
